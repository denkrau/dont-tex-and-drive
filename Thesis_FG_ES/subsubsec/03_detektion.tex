\subsubsection{Detektion}
\label{subsec:03detektion}
Als grundlegender Schritt der Personenverfolgung muss eine Person in dem Kamerabild gefunden werden. Dieser Vorgang nennt sich (Personen-)Detektion. Herkömmliche Histogram-of-Oriented-Gradients Ansätze liefern keine zufriedenstellende Lösung bezüglich des Autos. Daher fiel die Wahl auf die Nutzung eines Convolutional Neural Networks kurz CNN. Diese Netze eignen sich sehr gut zur Klassifizierung von Objekten in Bildern, da sie die Zugehörigkeit eines Objektes zu einer Klasse mit Wahrscheinlichkeiten ausdrücken.
Ein typisches CNN besteht aus drei Layern. Das erste ist das Convolutional Layer. Dazu wird das Kamerabild mit einem Kernel bzw. einer Faltungsmatrix gefaltet. Die Werte des Kernels sind dabei auf das jeweilige Problem angepasst. Es wurde somit eine sogenannte Feature-Map erzeugt. Werden nun die Ergebnisse der Faltung mit einer Aktivierungsfunktion verknüpft, bildet dies den Eingang der einzelnen Neuronen. Bei CNNs ist die Nutzung einer Rectified Linear Unit mit $f(x) = max(0,x)$ mit $x$ als Ergebnis der Faltung üblich. Die zweite Schicht bildet das Pooling Layer. Hier findet eine Ordnungsreduktion statt, indem überflüssige Informationen verworfen werden. Das geschieht beispielsweise mit einem Max-Pooling. Dabei wandert ein $2 \times 2$-Quadrat über die Neuronen des Convolutional Layers und behält nur die Aktivität des aktivsten Neurons. Der Vorteil dieses Layers ist unter anderem eine erhöhte Berechnungsgeschwindigkeit. Den Abschluss bildet das Fully-connected Layer. Hier werden die Eingänge des Layers Objektklassen zugeteilt.

Als Neuronales Netz zur Bewältigung der ursprünglichen Aufgabe wird das \textit{Google MobileNet SSD}\cite{mobilenet} als Caffe Framework Modell verwendet, welches bereits vortrainiert ist. Ein weiterer Vorteil dieses Netzes ist die Optimierung auf ressourcenschonende Berechnungen, allerdings auf Kosten der Genauigkeit. Dieser einhergehende Nachteil wird im vorliegenden Anwendungsfall jedoch nicht als relevant erkannt. Diese Optimierung zieht eine Änderung der gewöhnlichen Struktur eines CNNs mit sich wie in Abbildung \ref{fig:pv-neural-net} dargestellt.
\begin{figure}[btp]
	\centering
	\includegraphics[width=.6\textwidth]{./pics/pv-neural-net.png}
	\caption{Links: Standard Convolutional Layer mit Batch Normalization und ReLU. Rechts: Depthwise Separable Convolutions mit Depthwise und Pointwise Layers gefolgt von Batch Normalization und ReLU. (aus \cite{mobilenet})}
	\label{fig:pv-neural-net}
\end{figure}
Die grundlegende Idee hinter Depthwise Separable Convolution ist die Aufteilung der Faltung eines CNNs in eine $3 \times 3$ Depthwise Convolution gefolgt von einer $1 \times 1$ Pointwise Convolution. Dabei dient ersteres dazu, jeden Eingangkanal zu filtern und letzteres dazu, diese Ausgänge wieder zusammenzuführen. Der Vorteil davon ist die drastische Reduzierung des Berechnungsaufwands und der Modellgröße. Batch Normalization\footnote{https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c} bezeichnet dabei die Normalisierung der Werte der unsichtbaren Schichten des Netzes, um die Trainingsgeschwindigkeit deutlich zu erhöhen.

Benutzt wird das Netz mithilfe des seit OpenCV 3.3 existierenden Deep Neural Networks (dnn) Moduls. Dieses kann unter anderem Caffe Framework Modelle laden, benutzen und trainieren. Letztere Funktion wird nicht benutzt, da das Netz bereits ausreichend trainiert ist. Daher muss es lediglich einmalig zum Programmstart geladen werden. Der aktuelle Kameraframe des Farbbildes wird auf eine Größe von \SI{300}{px}$\times$\SI{300}{px} skaliert, da das Netz ein Eingangsbild dieser Größe erwartet. Anschließend wird der Frame vorverarbeitet. Einerseits wird mit einer so genannten Mean Subtraction vom Frame der Durchschnitt des jeweiligen Farbkanals subtrahiert, um Helligkeitsunterschieden entgegen zu wirken. Dabei wird der Durchschnitt hier auf allen Farbkanälen zu $\mu = 127,5$ gewählt. Andererseits kann diese Subtraktion mithilfe eines Skalierungsparameters $\sigma$ normalisiert werden. Hier wird $\sigma = 127,502231$ gewählt. Zu beachten ist, dass der übergebene Skalierungsfaktor der \textit{blobFromImage}-Funktion $1/\sigma$ entspricht. Jetzt kann der eben erstellte Blob dem Netz als Eingang übergeben werden. Die Ausgabe beinhaltet alle erkannten Objekte als Matrix zusammengefasst. Eine Zeile dieser Matrix besteht aus den Einträgen $d^T = [0, \text{ID der zugehörigen Klasse}, \text{Wahrscheinlichkeit}, x, y, \text{Breite}, \text{Höhe}]$. Dabei liegen die Werte der $x$- und $y$-Koordinate im Bereich $[0, 1]$ und müssen, um die eigentlichen Koordinaten zu erhalten, erst mit der Bildbreite bzw. Bildhöhe multipliziert werden. Da unter Umständen mehrere Personen im Bild detektiert werden können, wird diejenige ausgewählt, die die größte Klassenwahrscheinlichkeit besitzt. Eine Alternative wäre, zusätzlich die Wahl der Personen auf einen Bereich um den Bildmittelpunkt zu begrenzen. Nun ist die gewünschte Person detektiert und es kann mithilfe der angesprochenen Koordinatentransformation eine Bounding Box der Person für die nachfolgenden Module erstellt werden.