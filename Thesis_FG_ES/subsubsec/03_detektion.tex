\subsubsection{Detektion}
\label{subsec:03detektion}
Als grundlegender Schritt der Personenverfolgung muss eine Person in dem Kamerabild gefunden werden. Dieser Vorgang nennt sich (Personen-)Detektion. Herkömmliche Histogram-of-Oriented-Gradients Ansätze liefern keine zufriedenstellende Lösung bezüglich des Fahrzeugs. Daher fällt die Wahl auf die Nutzung eines Convolutional-Neural-Networks kurz CNN. Diese Netze eignen sich sehr gut zur Klassifizierung von Objekten in Bildern, da sie die Zugehörigkeit eines Objektes zu einer Klasse mit Wahrscheinlichkeiten ausdrücken.

Ein typisches CNN besteht aus drei Layern. Das erste ist das Convolutional Layer. Durch die Faltung des Kamerabildes mit einem Kernel bzw. einer Faltungsmatrix werden die Werte der Eingangsschicht berechnet. Die Werte des Kernels sind dabei auf das jeweilige Problem angepasst. Das Ergebnis davon ist eine sogenannte Feature-Map. Werden nun die Ergebnisse der Faltung mit einer Aktivierungsfunktion verknüpft, bildet dies den Ausgang der einzelnen Neuronen des ersten Layers. Bei CNNs ist die Nutzung einer Rectified Linear Unit (ReLU) definiert durch $f(x) = max(0,x)$ mit $x$ als Ergebnis der Faltung üblich. Die zweite Schicht bildet das Pooling Layer. Hier findet eine Ordnungsreduktion statt, indem überflüssige Informationen verworfen werden. Das geschieht beispielsweise mit einem Max-Pooling. Dabei wandert ein $2 \times 2$-Filter über den Ausgang der Neuronen des Convolutional Layers und behält nur die Aktivität des jeweils aktivsten Neurons. Der Vorteil dieses Layers ist unter anderem eine erhöhte Berechnungsgeschwindigkeit des gesamten Netzes. Den Abschluss bildet das Fully-connected Layer. Hier werden die Eingänge des Layers Objektklassen zugeteilt.

Als Neuronales Netz zur Bewältigung der ursprünglichen Aufgabe wird das \textit{Google MobileNet SSD}\cite{mobilenet} als Caffe Framework Modell verwendet, welches bereits vortrainiert ist. Ein weiterer Vorteil dieses Netzes ist die Optimierung auf ressourcenschonende Berechnungen, allerdings auf Kosten der Genauigkeit. Dieser einhergehende Nachteil wird im vorliegenden Anwendungsfall jedoch nicht als relevant erkannt. Diese Optimierung zieht eine Änderung der gewöhnlichen Struktur eines CNNs mit sich wie in Abbildung \ref{fig:pv-neural-net} dargestellt.
\begin{figure}[btp]
	\centering
	\includegraphics[width=.6\textwidth]{./pics/pv-neural-net.png}
	\caption{Links: Standard Convolutional Layer mit Batch Normalization und ReLU. Rechts: Depthwise Separable Convolutions mit Depthwise und Pointwise Layers gefolgt von Batch Normalization und ReLU. (aus \cite{mobilenet})}
	\label{fig:pv-neural-net}
\end{figure}
Die grundlegende Idee hinter Depthwise Separable Convolution ist die Aufteilung der Faltung eines CNNs in eine $3 \times 3$ Depthwise Convolution gefolgt von einer $1 \times 1$ Pointwise Convolution. Dabei dient Ersteres dazu, jeden Eingangkanal zu filtern und Letzteres dazu, diese Ausgänge wieder zusammenzuführen. Der Vorteil davon ist die drastische Reduzierung des Berechnungsaufwands und der Modellgröße. Batch Normalization\footnote{https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c} bezeichnet dabei die Normalisierung der Werte der unsichtbaren Schichten des Netzes, um die Trainingsgeschwindigkeit deutlich zu erhöhen.

Benutzt wird das Netz mithilfe des seit OpenCV 3.3 existierenden Deep Neural Networks (dnn) Moduls. Dieses kann unter anderem Caffe Framework Modelle laden, benutzen und trainieren. Letztere Funktion wird nicht benutzt, da das Netz bereits ausreichend trainiert ist. Daher muss es lediglich einmalig zum Programmstart geladen werden. Der aktuelle Kameraframe des Farbbildes wird auf eine Größe von \SI{300}{px}$\times$\SI{300}{px} skaliert, da das Netz ein Eingangsbild dieser Größe erwartet. Anschließend wird der Frame vorverarbeitet. Einerseits erfolgt mit einer sogenannten Mean Subtraction eine Subtraktion des Durchschnitts des jeweiligen Farbkanals vom Kameraframe, um Helligkeitsunterschieden entgegen zu wirken. Dabei wird hier der Durchschnitt auf allen Farbkanälen zu $\mu = 127,5$ gewählt. Andererseits kann diese Subtraktion mithilfe eines Skalierungsparameters $\sigma$ normalisiert werden. Hier wird $\sigma = 127,502231$ gewählt. Zu beachten ist, dass der an die \textit{blobFromImage}-Funktion übergebende Skalierungsfaktor $1/\sigma$ entspricht. Anschließend kann der eben erstellte Blob dem Netz als Eingang übergeben werden. Die Ausgabe beinhaltet alle erkannten Objekte als Matrix zusammengefasst. Eine Zeile dieser Matrix besteht aus den Einträgen $d^T = [0, \text{ID der zugehörigen Klasse}, \text{Wahrscheinlichkeit}, x, y, \text{Breite}, \text{Höhe}]$. Dabei liegen die Werte der $x$- und $y$-Koordinate im Bereich $[0, 1]$ und müssen, um die eigentlichen Koordinaten zu erhalten, erst mit der Bildbreite bzw. Bildhöhe multipliziert werden. Da unter Umständen mehrere Personen im Bild detektiert werden können, wird diejenige ausgewählt, die die größte Klassenwahrscheinlichkeit besitzt. Eine Alternative wäre, zusätzlich die Wahl der Personen auf einen Bereich um den Bildmittelpunkt zu begrenzen, wenn davon ausgegangen wird, dass die zu detektierende Person frontal vor dem Fahrzeug steht. Nun ist die gewünschte Person detektiert und es kann mithilfe der angesprochenen Koordinatentransformation eine Bounding-Box der Person für die nachfolgenden Module erstellt werden.